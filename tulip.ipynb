{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/barthelemymp/TULIP-TCR/blob/main/tulip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJiANGCdNplD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RsCjRIQyNpiS"
      },
      "outputs": [],
      "source": [
        "#@title Clone github repo\n",
        "import json, time, os, sys, glob\n",
        "\n",
        "if not os.path.isdir(\"TULIP-TCR\"):\n",
        "  os.system(\"git clone -q https://github.com/barthelemymp/TULIP-TCR.git\")\n",
        "sys.path.append('/content/TULIP-TCR/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm-58pdYOmKo",
        "outputId": "de8fd5f3-7672-4477-a9c9-aba0e6dd633e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.24.0\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (2.31.0)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.24.0) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, pyarrow-hotfix, dill, multiprocess, transformers, datasets\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed datasets-2.16.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6 tokenizers-0.13.3 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers==4.24.0  datasets  tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HvtYmEOiOmHm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "#from data import TranslationDataset\n",
        "from transformers import BertTokenizerFast, BertTokenizer\n",
        "from transformers import BertModel, BertForMaskedLM, BertConfig, EncoderDecoderModel, BertLMHeadModel, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertOnlyMLMHead, SequenceClassifierOutput\n",
        "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.modeling_outputs import ModelOutput\n",
        "\n",
        "from transformers import PretrainedConfig\n",
        "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n",
        "from transformers.models.encoder_decoder.configuration_encoder_decoder import EncoderDecoderConfig\n",
        "import warnings\n",
        "\n",
        "from src.multiTrans import ED_BertForSequenceClassification, TCRDataset, BertLastPooler, unsupervised_auc, train_unsupervised, eval_unsupervised, MyMasking, Tulip, get_logscore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLQ2Q0h-fFuv"
      },
      "source": [
        "# Load model and file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "VdKAfHVuU_Jw",
        "outputId": "3a993ac5-c95f-4aec-de04-ea735fa317fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading hyperparameter\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#filesname = list(uploaded.keys())[0]\n",
        "finetunedmodel = True #@param {type:\"boolean\"}\n",
        "with open(\"configs/shallow.config.json\", \"r\") as read_file:\n",
        "    print(\"loading hyperparameter\")\n",
        "    modelconfig = json.load(read_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcgvjAwSfPzZ"
      },
      "source": [
        "# run prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRGWVReaOmFH",
        "outputId": "06417426-d581-41bc-be6b-759bf65c83da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading models ..\n",
            "self.pad_token_id 1\n",
            "self.pad_token_id 1\n",
            "self.pad_token_id 1\n",
            "loaded\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Tulip(\n",
              "  (encoderA): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "      (position_embeddings): Embedding(114, 128)\n",
              "      (token_type_embeddings): Embedding(1, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-1): 2 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoderA): ED_BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "        (position_embeddings): Embedding(50, 128)\n",
              "        (token_type_embeddings): Embedding(1, 128)\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-1): 2 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
              "    (LMcls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertLastPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (encoderB): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "      (position_embeddings): Embedding(114, 128)\n",
              "      (token_type_embeddings): Embedding(1, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-1): 2 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoderB): ED_BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "        (position_embeddings): Embedding(50, 128)\n",
              "        (token_type_embeddings): Embedding(1, 128)\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-1): 2 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
              "    (LMcls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertLastPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (encoderE): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "      (position_embeddings): Embedding(114, 128)\n",
              "      (token_type_embeddings): Embedding(1, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-1): 2 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoderE): ED_BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30, 128, padding_idx=1)\n",
              "        (position_embeddings): Embedding(50, 128)\n",
              "        (token_type_embeddings): Embedding(1, 128)\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-1): 2 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=128, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
              "    (LMcls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (transform_act_fn): GELUActivation()\n",
              "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertLastPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (MLMHeadA): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (MLMHeadB): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (MLMHeadE): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=128, out_features=30, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
              "  (mhc_embeddings): Embedding(55, 128)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aatok/\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
        "\n",
        "if tokenizer.sep_token is None:\n",
        "    tokenizer.add_special_tokens({'sep_token': '<MIS>'})\n",
        "\n",
        "if tokenizer.cls_token is None:\n",
        "    tokenizer.add_special_tokens({'cls_token': '<CLS>'})\n",
        "\n",
        "if tokenizer.eos_token is None:\n",
        "    tokenizer.add_special_tokens({'eos_token': '<EOS>'})\n",
        "\n",
        "if tokenizer.mask_token is None:\n",
        "    tokenizer.add_special_tokens({'mask_token': '<MASK>'})\n",
        "\n",
        "\n",
        "\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<CLS> $A <EOS>\",\n",
        "    pair=\"<CLS> $A <MIS> $B:1 <EOS>:1\",\n",
        "    special_tokens=[\n",
        "        (\"<EOS>\", 2),\n",
        "        (\"<CLS>\", 3),\n",
        "        (\"<MIS>\", 4),\n",
        "    ],\n",
        ")\n",
        "\n",
        "mhctok = AutoTokenizer.from_pretrained(\"mhctok/\")\n",
        "\n",
        "vocabsize = len(tokenizer._tokenizer.get_vocab())\n",
        "mhcvocabsize = len(mhctok._tokenizer.get_vocab())\n",
        "print(\"Loading models ..\")\n",
        "\n",
        "max_length = 114\n",
        "encoder_config = BertConfig(vocab_size = vocabsize,\n",
        "                    max_position_embeddings = max_length,\n",
        "                    num_attention_heads = modelconfig[\"num_attn_heads\"],\n",
        "                    num_hidden_layers = modelconfig[\"num_hidden_layers\"],\n",
        "                    hidden_size = modelconfig[\"hidden_size\"],\n",
        "                    type_vocab_size = 1,\n",
        "                    pad_token_id =  tokenizer.pad_token_id)\n",
        "\n",
        "encoder_config.mhc_vocab_size  =mhcvocabsize\n",
        "\n",
        "encoderA = BertModel(config=encoder_config)\n",
        "encoderB = BertModel(config=encoder_config)\n",
        "encoderE = BertModel(config=encoder_config)\n",
        "\n",
        "max_length = 50\n",
        "decoder_config = BertConfig(vocab_size = vocabsize,\n",
        "                    max_position_embeddings = max_length,\n",
        "                    num_attention_heads = modelconfig[\"num_attn_heads\"],\n",
        "                    num_hidden_layers = modelconfig[\"num_hidden_layers\"],\n",
        "                    hidden_size = modelconfig[\"hidden_size\"],\n",
        "                    type_vocab_size = 1,\n",
        "                    is_decoder=True,\n",
        "                    pad_token_id =  tokenizer.pad_token_id)    # Very Important\n",
        "\n",
        "decoder_config.add_cross_attention=True\n",
        "\n",
        "decoderA = ED_BertForSequenceClassification(config=decoder_config)\n",
        "decoderA.pooler = BertLastPooler(config=decoder_config)\n",
        "decoderB = ED_BertForSequenceClassification(config=decoder_config)\n",
        "decoderB.pooler = BertLastPooler(config=decoder_config)\n",
        "decoderE = ED_BertForSequenceClassification(config=decoder_config)\n",
        "decoderE.pooler = BertLastPooler(config=decoder_config)\n",
        "# Define encoder decoder model\n",
        "model = Tulip(encoderA=encoderA,encoderB=encoderB,encoderE=encoderE, decoderA=decoderA, decoderB=decoderB, decoderE=decoderE)\n",
        "\n",
        "def count_parameters(mdl):\n",
        "    return sum(p.numel() for p in mdl.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)\n",
        "\n",
        "if finetunedmodel:\n",
        "    checkpoint = torch.load(\"model_weights/hla0201_finetuned/multiTCR_s_mhcXfintune2/pytorch_model.bin\")\n",
        "    model.load_state_dict(checkpoint, False)\n",
        "    print(\"loaded\")\n",
        "else:\n",
        "    checkpoint = torch.load(\"model_weights/pretrained/multiTCR_s_mhcX_2_below20out/pytorch_model.bin\")\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(\"loaded\")\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogkZ_4yVdQ7l",
        "outputId": "0033f991-180f-4024-8e45-dc2335dc1135"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wrPnrsTDO4mg"
      },
      "outputs": [],
      "source": [
        "filesname = \"data/VDJ_test_2.csv\"\n",
        "compute_auc=True\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgVmDdT6e6D5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qetoCbz8O4jw",
        "outputId": "df759185-475c-42e9-b627-831919a5519f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the data ...\n",
            "GILGFVFTL\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "src.multiTrans.ClassifCausalLMOutputWithCrossAttentions is not a dataclasss. This is a subclass of ModelOutput and so must use the @dataclass decorator.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m datasetPetideSpecific\u001b[38;5;241m=\u001b[39m TCRDataset(filesname, tokenizer, device,target_peptide\u001b[38;5;241m=\u001b[39mtarget_peptide, mhctok\u001b[38;5;241m=\u001b[39mmhctok)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_peptide)\n\u001b[1;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mget_logscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasetPetideSpecific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m ranks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(np\u001b[38;5;241m.\u001b[39margsort(scores))\n\u001b[0;32m      9\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR3a\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m datasetPetideSpecific\u001b[38;5;241m.\u001b[39malpha\n",
            "File \u001b[1;32mg:\\Programming\\UPythonSource\\TULIP-TCR\\src\\multiTrans.py:1169\u001b[0m, in \u001b[0;36mget_logscore\u001b[1;34m(dataset, model, ignore_index)\u001b[0m\n\u001b[0;32m   1166\u001b[0m clf_label \u001b[38;5;241m=\u001b[39m binder\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m   1167\u001b[0m labels \u001b[38;5;241m=\u001b[39m clf_label\n\u001b[1;32m-> 1169\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43malpha_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbeta_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpeptide_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43malpha_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbeta_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpeptide_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmhc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmhc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftm\u001b[39m(x):\n\u001b[0;32m   1173\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(x)\n",
            "File \u001b[1;32mc:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mg:\\Programming\\UPythonSource\\TULIP-TCR\\src\\multiTrans.py:641\u001b[0m, in \u001b[0;36mTulip.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_outputs, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, mhc, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m    640\u001b[0m labelsA \u001b[38;5;241m=\u001b[39m (labels, input_idsA)\n\u001b[1;32m--> 641\u001b[0m decoder_outputsA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderA(\n\u001b[0;32m    642\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_idsA,\n\u001b[0;32m    643\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_maskA,\n\u001b[0;32m    644\u001b[0m     encoder_hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mhc_encoded,encoder_hidden_statesB, encoder_hidden_statesE], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    645\u001b[0m     encoder_attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([mhc_attention_mask, attention_maskB, attention_maskE], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    646\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embedsA,\n\u001b[0;32m    647\u001b[0m     output_attentions \u001b[38;5;241m=\u001b[39m output_attentions,\n\u001b[0;32m    648\u001b[0m     output_hidden_states \u001b[38;5;241m=\u001b[39m output_hidden_states,\n\u001b[0;32m    649\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabelsA,\n\u001b[0;32m    650\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    651\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_valuesA,\n\u001b[0;32m    652\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_decoder,\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    655\u001b[0m pooled_outputA \u001b[38;5;241m=\u001b[39m decoder_outputsA\u001b[38;5;241m.\u001b[39mpooled_output\n\u001b[0;32m    657\u001b[0m labelsB \u001b[38;5;241m=\u001b[39m (labels, input_idsB)\n",
            "File \u001b[1;32mc:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mg:\\Programming\\UPythonSource\\TULIP-TCR\\src\\multiTrans.py:257\u001b[0m, in \u001b[0;36mED_BertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# print(self.pad_token_id)\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     lm_loss \u001b[38;5;241m=\u001b[39m loss_fct(shifted_prediction_scores\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size), labelsLM\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClassifCausalLMOutputWithCrossAttentions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlossCLS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlossCLS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclf_logits\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attentions\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\akesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\generic.py:325\u001b[0m, in \u001b[0;36mModelOutput.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m is_modeloutput_subclass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m!=\u001b[39m ModelOutput\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_modeloutput_subclass \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dataclass(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a dataclasss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This is a subclass of ModelOutput and so must use the @dataclass decorator.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n",
            "\u001b[1;31mTypeError\u001b[0m: src.multiTrans.ClassifCausalLMOutputWithCrossAttentions is not a dataclasss. This is a subclass of ModelOutput and so must use the @dataclass decorator."
          ]
        }
      ],
      "source": [
        "target_peptidesFinal = pd.read_csv(filesname)[\"peptide\"].unique()\n",
        "\n",
        "for target_peptide in target_peptidesFinal:\n",
        "    results = pd.DataFrame(columns=[\"CDR3a\", \"CDR3b\", \"peptide\", \"rank\"])\n",
        "    datasetPetideSpecific= TCRDataset(filesname, tokenizer, device,target_peptide=target_peptide, mhctok=mhctok)\n",
        "    print(target_peptide)\n",
        "    scores = -1*np.array(get_logscore(datasetPetideSpecific, model, ignore_index =  tokenizer.pad_token_id))\n",
        "    ranks = np.argsort(np.argsort(scores))\n",
        "    results[\"CDR3a\"] = datasetPetideSpecific.alpha\n",
        "    results[\"CDR3b\"] = datasetPetideSpecific.beta\n",
        "    results[\"peptide\"] = target_peptide\n",
        "    results[\"rank\"] = ranks\n",
        "    # print(results)\n",
        "    if compute_auc:\n",
        "      dl = torch.utils.data.DataLoader(dataset=datasetPetideSpecific, batch_size=1, shuffle=False, collate_fn=datasetPetideSpecific.all2allmhc_collate_function)\n",
        "      # print(unsupervised_auc(model,dl, tokenizer.pad_token_id))\n",
        "      auce = roc_auc_score(datasetPetideSpecific.binder, ranks)\n",
        "      print(auce)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKa3opZiO4hQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN7vhaeIMSeFLX6ZlWsTllv",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
